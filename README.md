# Robust_RAG_Pipeline

This project implements a customized Retrieval-Augmented Generation (RAG) pipeline designed to improve robustness to irrelevant context, enhance retrieval precision, and refine training methodologies. The primary motivation behind this work is to address two major limitations of large language models (LLMs): their tendency to hallucinate false information and their inability to access recent or dynamic data beyond their training corpus. By integrating external knowledge sources into their text generation process, Retrieval-Augmented Language Models (RALMs) significantly enhance factual accuracy and contextual relevance.

Designed for optimal performance on the Comprehensive RAG (CRAG) Benchmark, which rigorously evaluates RAG implementations across diverse knowledge domains and question types, this implementation introduces several key modifications inspired by recent research on retrieval robustness. Improvements include replacing Instruct-based models with Llama-3.2-3B, utilizing 4-bit quantization for memory efficiency, and enhancing retrieval processes to mitigate cascading reasoning errors. Additionally, custom training datasets were generated by fine-tuning modern GPT models deployed on Microsoft Azure, leading to a 63.5% increase in single-hop reasoning data.

Experimental results demonstrate a 19.4% improvement in overall accuracy and a 7.3% gain in exact accuracy compared to baseline RAG implementations. While the inclusion of a retrieval mechanism significantly reduced missing response rates to 0%, an increase in hallucination rates remains an ongoing challenge. Future enhancements will focus on fine-tuning instruction alignment, refining retrieval scoring methods beyond basic cosine similarity, and integrating multi-hop reasoning to improve comprehension of complex queries. Additional optimizations may involve vector databases for similarity computation and hyperparameter tuning to balance retrieval efficiency and accuracy.

By advancing robust retrieval methodologies, this project contributes to the evolution of large language models, paving the way for more reliable and context-aware generative AI applications. Continuous refinement of retrieval-augmented models will further improve precision, adaptability, and real-world applicability in knowledge-intensive NLP tasks.
